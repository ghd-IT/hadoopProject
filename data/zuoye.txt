create external table if not exists logs(
    ver string,
    s_time string,
    en string,
    u_ud string,
    u_mid string,
    u_sd string,
    c_time string,
    l string,
    b_iev string,
    b_rst string,
    p_url string,
    p_ref string,
    tt string,
    pl string,
    ip string,
    oid string,
    `on` string,
    cua string,
    cut string,
    pt string,
    ca string,
    ac string,
    kv_ string,
    du string,
    browserName string,
    browserVersion string,
    osName string,
    osVersion string,
    country string,
    province string,
    city string
    )
    partitioned by(month String,day string)
    row format delimited fields terminated by '\001'
    ;

alter database hive  character set latin1	
load data inpath '/ods/11/02/' into table logs partition(month='11',day='02');
create  function date_convert as 'com.yaxin.bigdata.analystic.hive.DateDimensionUdf' using jar 'hdfs://hdp01:9000/logs/udfjars/hadoopProject-1.0-SNAPSHOT.jar'

create table if not exists dw_en(
s_time bigint,
pl string,
ca string,
ac string
)
partitioned by(month string ,day string)
row format delimited fields terminated by ' '
stored as orc; 	

导入数据
from  logs
insert into table dw_en partition(month="11",day="02")
select s_time,pl,ca,ac
where month = "11" and day = "02"; 


create external table if not exists stats_event(
platform_dimension_id int,
date_dimension_id int,
event_dimension_id int,
times int,
create string
);

导入数据到stats_event
with tmp as (
select from_unixtime(cast(de.s_time/1000 as bigint),"yyyy-MM-dd") dt,
de.pl pl,de.ca ca,de.ac ac
from dw_en de
where pl is not null
and de.month="11" and de.day= "02"
)
from (
select pl as pl,dt, ca as ca ,ac as ac,count(1) as ct from tmp group by pl,dt,ca,ac union all
select pl as pl,dt, ca as ca ,'all' as ac,count(1) as ct from tmp group by pl,dt,ca union all
select 'all' as pl,dt, ca as ca ,ac as ac,count(1) as ct from tmp group by dt,ca,ac union all
select 'all' as pl,dt, ca as ca ,'all' as ac,count(1) as ct  from tmp group by dt,ca union all
select pl as pl,dt, 'all' as ca ,'all' as ac,count(1) as ct  from tmp group by pl,dt union all
select 'all' as pl,dt, 'all' as ca ,'all' as ac,count(1) as ct  from tmp group by dt 
) as tmp2
insert into stats_event
select date_convert(dt),platform_convert(pl),event_convert(ca,ac),sum(ct),dt
group by pl,dt,ca,ac
;

数据导入mysql
sqoop export --connect jdbc:mysql://hdp01:3306/test \
--username hive --password 123456 --table stats_event \
--export-dir 'hdfs://192.168.152.10:9000/user/hive/warehouse/gp1809.db/stats_event/*' \
--input-fields-terminated-by '\001' \
--update-mode allowinsert \
--update-key platform_dimension_id,date_dimension_id,event_dimension_id
;

创建shell脚本


#!/bin/bash

#./en.sh -n -m -d 2018-08-30
run_date=
until [$# -eq 0]
do 
if [$1'x' = '-dx']
then
shift
run_date=$1
fi
shift
done

if [ ${#run_date} !=10]
then 
run_date = `date -d "1 days ago" "+%Y-%m-%d"`
else
echo "$run_date"
fi
month=`date -d "$run_date" "+%m"`
day=`date -d "$run_date" "+%d"` 
echo "final running date is:${run_date},${month},${day}"
############
#run hql statment
###########

with tmp as (
select from_unixtime(cast(de.s_time/1000 as bigint),"yyyy-MM-dd") dt,
de.pl pl,de.ca ca,de.ac ac
from dw_en de
where pl is not null
and de.month="${month}" and de.day= "${day}"
)
from (
select pl as pl,dt, ca as ca ,ac as ac,count(1) as ct from tmp group by pl,dt,ca,ac union all
select pl as pl,dt, ca as ca ,'all' as ac,count(1) as ct from tmp group by pl,dt,ca union all
select 'all' as pl,dt, ca as ca ,ac as ac,count(1) as ct from tmp group by dt,ca,ac union all
select 'all' as pl,dt, ca as ca ,'all' as ac,count(1) as ct  from tmp group by dt,ca union all
select pl as pl,dt, 'all' as ca ,'all' as ac,count(1) as ct  from tmp group by pl,dt union all
select 'all' as pl,dt, 'all' as ca ,'all' as ac,count(1) as ct  from tmp group by dt 
) as tmp2
insert overwrite into stats_event
select date_convert(dt),platform_convert(pl),event_convert(ca,ac),sum(ct),dt
group by pl,dt,ca,ac
;

echo "run sqoop statment..."
sqoop export --connect jdbc:mysql://hdp01:3306/test \
--username hive --password 123456 --table stats_event \
--export-dir 'hdfs://192.168.152.10:9000/user/hive/warehouse/gp1809.db/stats_event/*' \
--input-fields-terminated-by '\001' \
--update-mode allowinsert \
--update-key platform_dimension_id,date_dimension_id,event_dimension_id
;
echo "event job is finished..."

1、create table if not exists dw_dp(
s_time bigint,
pl string,
p_url string,
u_ud string,
u_sd string
)
partitioned by(month string,day string)
row format delimited
fields terminated by ' '
stored as orc;



insert into table dw_dp partition(month='11',day='02')
select s_time,pl,p_url,u_ud,u_sd
from logs
where month="11" and day ="02"
and en ='e_pv'
;

create table dwa_dp(
pl string,
dt string,
col string,
ct int
);

CREATE TABLE stats_view_depth (
platform_dimension_id int,
data_dimension_id int,
kpi_dimension_id int,
pv1 int,
pv2 int,
pv3 int,
pv4 int,
pv5_10 int,
pv10_30 int,
pv30_60 int,
pv60pluss int,
created string
);

insert overwrite table dwa_dp
select pl,dt,pv,count(distinct u_ud)
from  (
select from_unixtime(cast(dd.s_time/1000 as bigint),"yyyy-MM-dd") dt,
dd.pl pl,
(case when count(dd.p_url)=1 then 'pv1'
when count(dd.p_url)=1 then 'pv1'
when count(dd.p_url)=2 then 'pv2'
when count(dd.p_url)=3 then 'pv3'
when count(dd.p_url)=4 then 'pv4'
when count(dd.p_url)<=10 then 'pv5_10'
when count(dd.p_url)<=30 then 'pv10_30'
when count(dd.p_url)<=60 then 'pv30_60'
else 'pv60pluss'
end )as pv,
dd.u_ud u_ud
from dw_dp dd
where dd.pl is not null
and dd.p_url is not null 
and dd.month="11" and dd.day ="02"
group by pl,s_time,u_ud
) as tmp
group by pl,dt,pv
;


insert overwrite table stats_view_depth
select date_convert(dt),platform_convert(pl),2,
sum(pv1),sum(pv2),sum(pv3),sum(pv4),sum(pv5_10),sum(pv10_30),
sum(pv30_60),sum(pv60pluss),dt
from(
select pl as pl,dt,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv1" union all
select pl as pl,dt,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv2" union all
select pl as pl,dt,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv3" union all
select pl as pl,dt,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv4" union all
select pl as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv5_10" union all
select pl as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4, 0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv10_30" union all
select pl as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60pluss from dwa_dp where col ="pv30_60" union all
select pl as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60pluss from dwa_dp where col ="pv60pluss" union all
select 'all' as pl,dt,ct as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv1" union all
select 'all' as pl,dt,0 as pv1,ct as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv2" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,ct as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv3" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,0 as pv3,ct as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv4" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,ct as pv5_10,0 as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv5_10" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4, 0 as pv5_10,ct as pv10_30,0 as pv30_60,0 as pv60pluss from dwa_dp where col ="pv10_30" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,ct as pv30_60,0 as pv60pluss from dwa_dp where col ="pv30_60" union all
select 'all' as pl,dt,0 as pv1,0 as pv2,0 as pv3,0 as pv4,0 as pv5_10,0 as pv10_30,0 as pv30_60,ct as pv60pluss from dwa_dp where col ="pv60pluss" 
)tmp 
group by pl,dt
;


sqoop export --connect jdbc:mysql://hdp01:3306/test \
--username hive --password 123456 --table stats_view_depth \
--export-dir 'hdfs://192.168.152.10:9000/user/hive/warehouse/gp1809.db/stats_view_depth/*' \
--input-fields-terminated-by '\001' \
--update-mode allowinsert \
--update-key platform_dimension_id,date_dimension_id,kpi_dimension_id





create table order_info(
order_id string,
platform string,
s_time string,
currency_type string,
payment_type string,
amount int
);

insert overwrite table order_info
select distinct * from (
select oid,pl,from_unixtime(cast(s_time/1000 as bigint),"yyyy-MM-dd"),cut,pt,sum(cua)
from logs
where pl is not null
and oid!="null"
and month="11" and day ="02"
group by oid,pl,s_time,cut,pt
) tmp



sqoop export --connect jdbc:mysql://hdp01:3306/test \
--username hive --password 123456 --table order_info \
--export-dir 'hdfs://192.168.152.10:9000/user/hive/warehouse/gp1809.db/order_info/*' \
--input-fields-terminated-by '\001' \
--update-mode allowinsert 






CREATE TABLE `stats_order` (
  `platform_dimension_id` int,
  `date_dimension_id` int ,
  `currency_type_dimension_id` int,
  `payment_type_dimension_id` int,
  `orders` int,
  `success_orders` int,
  `refund_orders` int,
  `order_amount` int,
  `revenue_amount` int,
  `refund_amount` int,
  `total_revenue_amount` int,
  `total_refund_amount` int,
  `created` date  
);


CREATE TABLE `stats_order` (
  `platform_dimension_id` int(11) NOT NULL DEFAULT '0',
  `date_dimension_id` int(11) NOT NULL DEFAULT '0',
  `currency_type_dimension_id` int(11) NOT NULL DEFAULT '0',
  `payment_type_dimension_id` int(11) NOT NULL DEFAULT '0',
  `orders` int(11) DEFAULT '0' COMMENT '订单个数',
  `success_orders` int(11) DEFAULT '0' COMMENT '成功支付的订单个数',
  `refund_orders` int(11) DEFAULT '0' COMMENT '退款订单个数',
  `order_amount` int(11) DEFAULT '0' COMMENT '订单金额',
  `revenue_amount` int(11) DEFAULT '0' COMMENT '收入金额，也就是成功支付过的金额',
  `refund_amount` int(11) DEFAULT '0' COMMENT '退款金额',
  `total_revenue_amount` int(11) DEFAULT '0' COMMENT '迄今为止，总的订单交易额',
  `total_refund_amount` int(11) DEFAULT '0' COMMENT '迄今为止，总的退款金额',
  `created` date DEFAULT NULL,
  PRIMARY KEY (`platform_dimension_id`,`date_dimension_id`,`currency_type_dimension_id`,`payment_type_dimension_id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='统计订单信息的统计表';

create table dw_or(
pl string,
s_time string,
en string,
oid string,
cut string,
pt string,
cua int
);

insert into table dw_or
select pl,s_time,en,oid,cut,pt,cua
from logs
where en="e_cs" or en ="e_cr" or en="e_crt"


insert into table stats_order
select  b.pl,b.da,b.cut,b.pt,b.oid ,b.cs_en,b.cr_en,b.cua,(case when b.cs_cua is null then 0 else b.cs_cua end),(case when b.cr_cua is null then 0 else b.cr_cua end),sum(case when b.cs_cua is null then 0 else b.cs_cua end) over(distribute by  b.pl sort by b.da),sum(case when b.cr_cua is null then 0 else b.cr_cua end) over(distribute by  b.pl sort by b.da), b.dt
from(
select platform_convert(pl) pl,date_convert(dt) da,currency_type_convert(cut) cut,payment_type_convert(pt) pt,count(oid) oid ,count(cs_en) cs_en ,count(cr_en) cr_en ,sum(cua) cua,sum(cs_cua) cs_cua,sum(cr_cua) cr_cua , dt
from (select pl as pl,from_unixtime(cast(s_time/1000 as bigint),"yyyy-MM-dd") dt,cut as cut,pt as pt,oid as oid ,(case when en="e_cs" then en end) as cs_en  ,(case when en="e_cr" then en end) as cr_en,cua as cua ,(case when en="e_cs" then cua end) as cs_cua ,(case when en="e_cr" then cua end) as cr_cua 
from dw_or dd) as a
group by pl,dt,cut,pt ) as b 



sqoop export --connect jdbc:mysql://hdp01:3306/test \
--username hive --password 123456 --table stats_order \
--export-dir 'hdfs://192.168.152.10:9000/user/hive/warehouse/gp1809.db/stats_order/*' \
--input-fields-terminated-by '\001' \
--update-mode allowinsert \
--update-key platform_dimension_id,date_dimension_id,currency_type_dimension_id,payment_type_dimension_id 










